{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formación de sintagmas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*********************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables:\n",
    "\n",
    "$w=[w_1, ..., w_n] \\rightarrow$ El sintagma\n",
    "\n",
    "* $e_t \\in \\{0,1\\} \\rightarrow$ Ocurrencia del evento en el momento $t$\n",
    "* $s_t \\in \\{0,1\\} \\rightarrow$ Existencia de la situación en el momento $t$\n",
    "* $s_{t-1} \\rightarrow$ Situación en el momento $t-1$\n",
    "* $w_t^i \\rightarrow$ El evento del sintagma siendo formado en el momento $t$:\n",
    "    * $i=0$: hipótesis de que el sintagma no se ha formado aún\n",
    "    * $i=1$: hipótesis de que el sintagma se ha formado\n",
    "* $\\pi_t \\rightarrow$ Observación\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*********************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $\\alpha_j^i(t)=P[w_t=i|s_t=j]$\n",
    "\n",
    "* $\\beta_{kh}^j(t)=P[s_t=j|s_{t-1}=k,e_t=h]$\n",
    "\n",
    "* $\\nu^k(t)=P[s_{t-1}=k]$\n",
    "\n",
    "* $\\gamma^h(t)=P[e_t=h]$\n",
    "\n",
    "* $\\tau_i^{jkh}=P(s_t=j,s_{t-1}=k, e_t=h|w=i)$\n",
    "\n",
    "$i, j, k, h \\in \\{0,1\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*********************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consideraciones previas:\n",
    "\n",
    "- La situación no se ha dado en $t=0$\n",
    "    - $\\nu^0(t)=1$\n",
    "    - $\\nu^1(t)=0$\n",
    "\n",
    "- $\\psi$: lista que, para cada t, contiene la probabilidad de que el evento que causa el sintagma se genere\n",
    "\n",
    "- $\\alpha, \\beta, \\nu, \\gamma$ son listas de:\n",
    "\n",
    "    - n listas, siendo n el subíndice:\n",
    "        - cada lista de m posiciones según el superíndice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- $\\alpha$: [[ , ], [ , ]]\n",
    "- $\\beta$: [[ [ , ] , [ , ] ], [ [ , ] , [ , ] ]]\n",
    "- $\\nu$: una lista de dos posiciones\n",
    "- $\\gamma$: una lista de dos posiciones\n",
    "\n",
    "- $\\tau$: ocho listas de dos posiciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math\n",
    "import random\n",
    "\n",
    "EXT = \".json\"\n",
    "\n",
    "MONTHS = [\"Enero\", \"Febrero\", \"Marzo\", \"Abril\", \"Mayo\", \"Junio\", \"Julio\", \"Agosto\", \"Septiembre\", \"Octubre\", \"Noviembre\", \"Diciembre\"]\n",
    "ALL_MONTHS = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
    "HALF_MONTHS = [1, 3, 5, 7, 9, 11]\n",
    "LAST_DECADE = [2019, 2020, 2021, 2022, 2023]\n",
    "\n",
    "DATA_PATH = \"data/\"\n",
    "\n",
    "# Elementos importantes de los articulo\n",
    "RESPONSE = \"response\"\n",
    "DOCS = \"docs\"\n",
    "DATA = \"data\"\n",
    "\n",
    "ABSTRACT = \"abstract\"\n",
    "SNIPPET = \"snippet\"\n",
    "LEAD_PAR = \"lead_paragraph\"\n",
    "PUB_DATE = \"pub_date\"\n",
    "\n",
    "# Para la grafica\n",
    "YEAR = \"year\"\n",
    "MONTH = \"month\"\n",
    "FREQ_ALL_SYNTAGM = \"Complete Syntagm\"\n",
    "FREQ_SUM_SYNTAGMS = \"Divided Syntagm\"\n",
    "FREQ = \"frequency\"\n",
    "DATE = \"date\"\n",
    "\n",
    "ZERO_ONE = (0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotOnlyCompleteSyntagm(data, syntagm, interval, label):\n",
    "\n",
    "    if FREQ_ALL_SYNTAGM not in data:\n",
    "        print(f\"[ERROR plotOnlyCompleteSyntagm] Missing some flag {FREQ_ALL_SYNTAGM}\")\n",
    "        return\n",
    "\n",
    "    # Crear un DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Crear una columna de fecha combinando año y mes\n",
    "    df[PUB_DATE] = pd.to_datetime(df[[YEAR, MONTH]].assign(day=1))\n",
    "\n",
    "    # Establecer PUB_DATE como el índice del DataFrame\n",
    "    df.set_index(PUB_DATE, inplace=True)\n",
    "\n",
    "    # Crear la gráfica\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(df.index, df[FREQ_ALL_SYNTAGM], marker='o', label=label)\n",
    "\n",
    "    # Formatear el eje X para mostrar año y mes\n",
    "    plt.gca().xaxis.set_major_formatter(plt.matplotlib.dates.DateFormatter('%Y-%m'))\n",
    "    plt.gca().xaxis.set_major_locator(plt.matplotlib.dates.MonthLocator(interval=interval))  # Etiquetas cada 6 meses\n",
    "\n",
    "    # Rotar las etiquetas del eje X para mejor legibilidad\n",
    "    plt.gcf().autofmt_xdate(rotation=45)\n",
    "\n",
    "    # Añadir etiquetas y título\n",
    "    plt.xlabel(DATE)\n",
    "    plt.ylabel(FREQ)\n",
    "    plt.title(syntagm)\n",
    "\n",
    "    # Mostrar la gráfica\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plotAllData(data, syntagm, interval):\n",
    "\n",
    "    if FREQ_ALL_SYNTAGM not in data and FREQ_SUM_SYNTAGMS not in data:\n",
    "        print(\"[ERROR plotAllData] Missing some flag\")\n",
    "        return\n",
    "\n",
    "    # Crear un DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Crear una columna de fecha combinando año y mes\n",
    "    df[PUB_DATE] = pd.to_datetime(df[[YEAR, MONTH]].assign(day=1))\n",
    "\n",
    "    # Establecer PUB_DATE como el índice del DataFrame\n",
    "    df.set_index(PUB_DATE, inplace=True)\n",
    "\n",
    "    # Crear la gráfica\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(df.index, df[FREQ_ALL_SYNTAGM], marker='o', label=FREQ_ALL_SYNTAGM)\n",
    "    plt.plot(df.index, df[FREQ_SUM_SYNTAGMS], marker='o', label=FREQ_SUM_SYNTAGMS)\n",
    "\n",
    "    # Formatear el eje X para mostrar año y mes\n",
    "    plt.gca().xaxis.set_major_formatter(plt.matplotlib.dates.DateFormatter('%Y-%m'))\n",
    "    plt.gca().xaxis.set_major_locator(plt.matplotlib.dates.MonthLocator(interval=interval))  # Etiquetas cada 6 meses\n",
    "\n",
    "    # Rotar las etiquetas del eje X para mejor legibilidad\n",
    "    plt.gcf().autofmt_xdate(rotation=45)\n",
    "\n",
    "    # Añadir etiquetas y título\n",
    "    plt.xlabel(DATE)\n",
    "    plt.ylabel(FREQ)\n",
    "    plt.title(syntagm)\n",
    "\n",
    "    # Mostrar la gráfica\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plotAllDataTwoFigs(data, syntagm, interval):\n",
    "\n",
    "    if FREQ_ALL_SYNTAGM not in data and FREQ_SUM_SYNTAGMS not in data:\n",
    "        print(\"[ERROR plotAllDataTwoFigs] Missing some flag\")\n",
    "        return\n",
    "\n",
    "    # Crear un DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Crear una columna de fecha combinando año y mes\n",
    "    df[PUB_DATE] = pd.to_datetime(df[[YEAR, MONTH]].assign(day=1))\n",
    "\n",
    "    # Establecer PUB_DATE como el índice del DataFrame\n",
    "    df.set_index(PUB_DATE, inplace=True)\n",
    "\n",
    "    figure, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "    # Crear las gráficas\n",
    "    ax1.plot(df.index, df[FREQ_ALL_SYNTAGM], marker='o', label=FREQ_ALL_SYNTAGM, color='blue')\n",
    "    ax1.set_xlabel(DATE)\n",
    "    ax1.set_ylabel(FREQ)\n",
    "    ax1.set_title(syntagm)\n",
    "    ax1.legend()\n",
    "\n",
    "    ax2.plot(df.index, df[FREQ_SUM_SYNTAGMS], marker='o', label=FREQ_SUM_SYNTAGMS, color='orange')\n",
    "    ax2.set_xlabel(DATE)\n",
    "    ax2.set_ylabel(FREQ)\n",
    "    ax2.set_title(syntagm)\n",
    "    ax2.legend()\n",
    "\n",
    "    # Formatear el eje X para mostrar año y mes\n",
    "    ax1.xaxis.set_major_formatter(plt.matplotlib.dates.DateFormatter('%Y-%m'))\n",
    "    ax1.xaxis.set_major_locator(plt.matplotlib.dates.MonthLocator(interval=interval))\n",
    "    \n",
    "    ax2.xaxis.set_major_formatter(plt.matplotlib.dates.DateFormatter('%Y-%m'))\n",
    "    ax2.xaxis.set_major_locator(plt.matplotlib.dates.MonthLocator(interval=interval))\n",
    "\n",
    "    # Rotar las etiquetas del eje X para mejor legibilidad\n",
    "    plt.gcf().autofmt_xdate(rotation=45)\n",
    "\n",
    "    # Mostrar la gráfica\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Normaliza pi en un instante t\n",
    "def normPi(pi: np.ndarray):\n",
    "    if sum(pi) == 0:\n",
    "        return np.array([0, 0])\n",
    "    return pi / sum(pi)\n",
    "\n",
    "\n",
    "def computePi(year: int, month: int, syntagm: str):\n",
    "\n",
    "    # contador para las apariciones del sintagma completo\n",
    "    count_syntagm = 0\n",
    "\n",
    "    # diccionario contador para las apariciones de cada una de las palabras\n",
    "    split = syntagm.split(\" \")\n",
    "    count_individual_words = dict()\n",
    "\n",
    "    if len(split) > 1:\n",
    "        for word in split:\n",
    "            count_individual_words[word] = 0\n",
    "\n",
    "    # ruta al json de datos\n",
    "    path = DATA_PATH + str(year) + \"/\" + str(month) + EXT\n",
    "\n",
    "    with open(path, \"r\") as f:\n",
    "\n",
    "        # cargamos el json en memoria\n",
    "        docs = json.loads(f.read())[DATA]\n",
    "\n",
    "        for doc in docs:\n",
    "\n",
    "            abstract = doc[ABSTRACT]\n",
    "            snippet = doc[SNIPPET]\n",
    "            lead = doc[LEAD_PAR]\n",
    "\n",
    "            regex = fr\"\\b{syntagm}\\b\"\n",
    "\n",
    "            num_abstract = len(re.findall(regex, abstract, re.IGNORECASE))\n",
    "            num_snippet = len(re.findall(regex, snippet, re.IGNORECASE))\n",
    "            num_lead = len(re.findall(regex, lead, re.IGNORECASE))\n",
    "\n",
    "            # contamos el numero de apariciones del sintagma completo\n",
    "            count_syntagm += num_abstract + num_snippet + num_lead\n",
    "\n",
    "            if len(split) > 1:\n",
    "\n",
    "                # contamos el numero de apariciones de cada termino del sintagma\n",
    "                # mediante una expresion regular\n",
    "                for word in count_individual_words:\n",
    "                    regex = fr\"\\b{word}\\b\"\n",
    "\n",
    "                    num_abstract = len(re.findall(regex, abstract, re.IGNORECASE))\n",
    "                    num_snippet = len(re.findall(regex, snippet, re.IGNORECASE))\n",
    "                    num_lead = len(re.findall(regex, lead, re.IGNORECASE))\n",
    "\n",
    "                    count_individual_words[word] += num_abstract + num_snippet + num_lead\n",
    "\n",
    "    if len(split) > 1:\n",
    "        return np.array([count_syntagm, sum(count_individual_words.values())])\n",
    "    else:\n",
    "        return np.array([count_syntagm, count_syntagm])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Posibles sintagmas que se me van ocurriendo:\n",
    "#   bitcoin, cryptocurrency, brexit, internet\n",
    "\n",
    "def computeData(syntagm, years, months):\n",
    "\n",
    "    data = dict()\n",
    "\n",
    "    data[YEAR] = list()\n",
    "    data[MONTH] = list()\n",
    "    data[FREQ_ALL_SYNTAGM] = list()\n",
    "    data[FREQ_SUM_SYNTAGMS] = list()\n",
    "\n",
    "    for i in years:\n",
    "        for j in months:\n",
    "\n",
    "            data[YEAR].append(i)\n",
    "            data[MONTH].append(j)\n",
    "\n",
    "            pi = computePi(i, j, syntagm)\n",
    "            # pi = normPi(pi)\n",
    "\n",
    "            data[FREQ_ALL_SYNTAGM].append(pi[0])\n",
    "            data[FREQ_SUM_SYNTAGMS].append(pi[1])\n",
    "\n",
    "    return data\n",
    "\n",
    "years = [2019, 2020, 2021, 2022, 2023]\n",
    "months = ALL_MONTHS\n",
    "syntagm = \"lockdown\"\n",
    "\n",
    "data = computeData(syntagm, years, months)\n",
    "print(data)\n",
    "\n",
    "interval = 3\n",
    "plotOnlyCompleteSyntagm(data, syntagm, interval, FREQ_ALL_SYNTAGM)\n",
    "# plotAllData(data, syntagm, interval)\n",
    "# plotAllDataTwoFigs(data, syntagm, interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def normAlpha(alpha: np.ndarray):\n",
    "    return np.array([[alpha[j][i]/max(0.01, alpha[j].sum()) for i in [0, 1]] for j in [0, 1]])\n",
    "\n",
    "\n",
    "def computeAlpha(alpha: np.ndarray, tau: np.ndarray, pi: np.ndarray, t: int):\n",
    "    for i in ZERO_ONE:\n",
    "        for j in ZERO_ONE:\n",
    "            alpha[j][i] = pi[i][t] * sum(tau[i][j].flatten())\n",
    "    return alpha\n",
    "\n",
    "\n",
    "def normBeta(beta: np.ndarray):\n",
    "    return np.array([[[beta[h][k][j]/max(0.01, beta[h][k].sum()) for j in ZERO_ONE] for k in ZERO_ONE] for h in ZERO_ONE])\n",
    "\n",
    "\n",
    "def computeBeta(beta: np.ndarray, tau: np.ndarray, pi: np.ndarray, t: int):\n",
    "    for j in ZERO_ONE:\n",
    "        for k in ZERO_ONE:\n",
    "            for h in ZERO_ONE:\n",
    "                beta[h][k][j] = pi[0][t] * tau[0][j][k][h] + \\\n",
    "                    pi[1][t] * tau[1][j][h][k]\n",
    "    return beta\n",
    "\n",
    "\n",
    "def normGamma(gamma: np.ndarray):\n",
    "    return np.array([gamma[i]/max(0.01, gamma.sum()) for i in ZERO_ONE])\n",
    "\n",
    "\n",
    "def computeGamma(gamma: np.ndarray, tau: np.ndarray, pi: np.ndarray, t: int):\n",
    "\n",
    "    for h in ZERO_ONE:\n",
    "        g = 0\n",
    "        for i in ZERO_ONE:\n",
    "            for j in ZERO_ONE:\n",
    "                for k in ZERO_ONE:\n",
    "                    g += pi[i][t] * tau[i][j][k][h]\n",
    "\n",
    "        gamma[h] = g\n",
    "\n",
    "    return gamma\n",
    "\n",
    "def normTau(tau: np.ndarray):\n",
    "    return np.array([[[[tau[i][j][k][h]/max(0.01, tau[i].flatten().sum()) for h in ZERO_ONE] for k in ZERO_ONE] for j in ZERO_ONE] for i in ZERO_ONE])\n",
    "\n",
    "\n",
    "def computeTau(tau: np.ndarray, alpha: np.ndarray, beta: np.ndarray, nu: np.ndarray, gamma: np.ndarray, t: int):\n",
    "\n",
    "    for h in ZERO_ONE:\n",
    "        for k in ZERO_ONE:\n",
    "            for j in ZERO_ONE:\n",
    "                for i in ZERO_ONE:\n",
    "\n",
    "                    tau[h][k][j][i] = alpha[j][i] * \\\n",
    "                        beta[h][k][j] * nu[k][t-1] * gamma[h]\n",
    "    return tau\n",
    "\n",
    "\n",
    "def computeNu(nu: np.ndarray, beta: np.ndarray, gamma: np.ndarray, t: int):\n",
    "    s = [0, 0]\n",
    "    for h in ZERO_ONE:\n",
    "        for k in ZERO_ONE:\n",
    "            s[0] += beta[h][k][0] * nu[k][t-1] * gamma[h]\n",
    "            s[1] += beta[h][k][1] * nu[k][t-1] * gamma[h]\n",
    "\n",
    "    nu[0][t] = s[0]\n",
    "    nu[1][t] = s[1]\n",
    "\n",
    "    return nu\n",
    "\n",
    "\n",
    "def dst(a: np.ndarray, b: np.ndarray):\n",
    "\n",
    "    if a is None or b is None:\n",
    "        return 100000.0\n",
    "    else:\n",
    "        a = a.tolist()\n",
    "        b = b.tolist()\n",
    "        return math.sqrt(sum([(x-y)**2 for (x, y) in zip(a, b)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ER(T, pi, Tol, Nmax):\n",
    "    Psi = []\n",
    "\n",
    "    nu = np.zeros(shape=(2, T+1))\n",
    "    nu[0][-1] = 0.95\n",
    "    nu[1][-1] = 0.05\n",
    "\n",
    "    for t in range(T):\n",
    "        prev = None\n",
    "        iter = 0\n",
    "\n",
    "        # Inicializar y normalizar \\alpha, \\beta, \\gamma, \\tau\n",
    "        alpha: np.ndarray = np.random.rand(2, 2)\n",
    "        alpha = normAlpha(alpha)\n",
    "\n",
    "        beta: np.ndarray = np.random.rand(2, 2, 2)\n",
    "        beta = normBeta(beta)\n",
    "\n",
    "        gamma: np.ndarray = np.random.rand(2)\n",
    "        gamma = normGamma(gamma)\n",
    "\n",
    "        tau: np.ndarray = np.random.rand(2, 2, 2, 2)\n",
    "        tau = normTau(tau)\n",
    "\n",
    "        elst = np.array([])\n",
    "        elst = np.append(elst, alpha)\n",
    "        elst = np.append(elst, beta)\n",
    "        elst = np.append(elst, gamma)\n",
    "        elst = np.append(elst, tau)\n",
    "\n",
    "        while iter < Nmax and dst(elst, prev)/max(0.01, dst(elst, np.zeros(elst.shape))) > Tol:\n",
    "            iter += 1\n",
    "            prev = elst[:]\n",
    "\n",
    "            tau = computeTau(tau, alpha, beta, nu, gamma, t)\n",
    "            tau = normTau(tau)\n",
    "\n",
    "            alpha = computeAlpha(alpha, tau, pi, t)\n",
    "            alpha = normAlpha(alpha)\n",
    "\n",
    "            beta = computeBeta(beta, tau, pi, t)\n",
    "            beta = normBeta(beta)\n",
    "\n",
    "            gamma = computeGamma(gamma, tau, pi, t)\n",
    "            gamma = normGamma(gamma)\n",
    "\n",
    "            elst = np.array([])\n",
    "            elst = np.append(elst, alpha)\n",
    "            elst = np.append(elst, beta)\n",
    "            elst = np.append(elst, gamma)\n",
    "            elst = np.append(elst, tau)\n",
    "\n",
    "        # print(alpha.flatten())\n",
    "        # print(beta.flatten())\n",
    "        # print(gamma.flatten())\n",
    "        # print(tau.flatten())\n",
    "        # print()\n",
    "        # print()\n",
    "\n",
    "        nu = computeNu(nu, beta, gamma, t)\n",
    "\n",
    "        Psi += [gamma[1]]\n",
    "\n",
    "    return [Psi[0]] + [max(Psi[i]-Psi[i-1], 0) for i in range(1, len(Psi))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(lst):\n",
    "    res = []\n",
    "    for ele in lst:\n",
    "        if type(ele) == list:\n",
    "            res += flatten(ele)\n",
    "        else:\n",
    "            res += [ele]\n",
    "    return res\n",
    "\n",
    "\n",
    "#\n",
    "# Distance between two vectors\n",
    "#\n",
    "def dst(a, b):\n",
    "    if a is None or b is None:\n",
    "        return 100000.0\n",
    "    else:\n",
    "        return math.sqrt(sum([(x-y)**2 for (x, y) in zip(a, b)]))\n",
    "\n",
    "\n",
    "#\n",
    "# normalizes alpha, beta, tau\n",
    "#\n",
    "def alphanorm(alpha):\n",
    "    return [[alpha[j][i]/max(0.01, sum(alpha[j])) for i in ZERO_ONE] for j in ZERO_ONE]\n",
    "\n",
    "\n",
    "def betanorm(beta):\n",
    "    return [[[beta[h][k][j]/max(0.01, sum(beta[h][k])) for j in ZERO_ONE] for k in ZERO_ONE] for h in ZERO_ONE]\n",
    "\n",
    "\n",
    "def taunorm(tau):\n",
    "    return [[[[tau[i][j][k][h]/max(0.01, sum(flatten(tau[i]))) for h in ZERO_ONE] for k in ZERO_ONE] for j in ZERO_ONE ] for i in ZERO_ONE]\n",
    "\n",
    "def gammanorm(gamma):\n",
    "    return [gamma[h]/max(0.01, sum(gamma)) for h in ZERO_ONE]\n",
    "\n",
    "def ER_santini(T, Pi, Tol, Nmax):\n",
    "    Psi = []\n",
    "    #\n",
    "    # Nu[i][t]: Prob(s(t)=i)\n",
    "    #\n",
    "    Nu = [[0 for _ in range(T+1)] for _ in range(2)]\n",
    "    Nu[0][-1] = 0.95\n",
    "    Nu[1][-1] = 0.05\n",
    "    for t in range(T):\n",
    "        prev = None\n",
    "        iter = 0\n",
    "\n",
    "        alpha = [[random.uniform(0, 1) for i in ZERO_ONE] for j in ZERO_ONE]\n",
    "        alpha = alphanorm(alpha)\n",
    "\n",
    "        beta = [[[random.uniform(0, 1) for j in ZERO_ONE] for k in ZERO_ONE] for h in ZERO_ONE]\n",
    "        beta = betanorm(beta)\n",
    "\n",
    "        gamma = [random.uniform(0, 1) for h in ZERO_ONE]\n",
    "        gamma = [ gamma[h]/max(0.01, sum(gamma)) for h in ZERO_ONE]\n",
    "\n",
    "        tau = [[[[ random.uniform(0, 1) for h in ZERO_ONE] for k in ZERO_ONE] for j in ZERO_ONE ] for i in ZERO_ONE]\n",
    "        tau = taunorm(tau)\n",
    "\n",
    "        elst = flatten(alpha) + flatten(beta) + flatten(gamma) + flatten(tau)\n",
    "        while iter < Nmax and dst(elst, prev)/max(0.01, dst(elst, len(elst)*[0.0])) > Tol:\n",
    "            iter += 1\n",
    "            prev = elst[:]\n",
    "\n",
    "            # print(Nu)\n",
    "            tau = [[[[alpha[j][i]*beta[h][k][j]*Nu[k][t-1]*gamma[h] for i in ZERO_ONE] for j in ZERO_ONE] for k in ZERO_ONE ] for h in ZERO_ONE]\n",
    "            # print(tau)\n",
    "            tau = taunorm(tau)\n",
    "            \n",
    "\n",
    "            alpha = [[Pi[i][t]*sum(flatten(tau[i][j])) for i in ZERO_ONE] for j in ZERO_ONE]\n",
    "            alpha = alphanorm(alpha)\n",
    "\n",
    "            for j in ZERO_ONE:\n",
    "                for k in ZERO_ONE:\n",
    "                    for h in ZERO_ONE:\n",
    "                        beta[h][k][j] = Pi[0][t]*tau[0][j][k][h]+Pi[1][t]*tau[1][j][h][k]\n",
    "            beta = betanorm(beta)\n",
    "\n",
    "            for h in ZERO_ONE:\n",
    "                s = 0\n",
    "                for i in ZERO_ONE:\n",
    "                    for j in ZERO_ONE:\n",
    "                        for k in ZERO_ONE:\n",
    "                            s += Pi[i][t]*tau[i][j][k][h]\n",
    "                gamma[h] = s\n",
    "            gamma = [gamma[h]/max(0.01, sum(gamma)) for h in ZERO_ONE]\n",
    "            elst = flatten(alpha) + flatten(beta) + flatten(gamma) + flatten(tau)\n",
    "            \n",
    "        # print(flatten(alpha))\n",
    "        # print(flatten(beta))\n",
    "        # print(flatten(gamma))\n",
    "        # print(flatten(tau))\n",
    "        # print()\n",
    "        # print()\n",
    "        \n",
    "        s = [0, 0]\n",
    "        for h in ZERO_ONE:\n",
    "            for k in ZERO_ONE:\n",
    "                s[0] += beta[h][k][0]*Nu[k][t-1]*gamma[h]\n",
    "                s[1] += beta[h][k][1]*Nu[k][t-1]*gamma[h]\n",
    "        Nu[0][t] = s[0]\n",
    "        Nu[1][t] = s[1]\n",
    "        Psi += [gamma[1]]\n",
    "    return [Psi[0]] + [max(Psi[i]-Psi[i-1], 0) for i in range(1, len(Psi))]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# years = [2015, 2016, 2017, 2018, 2019]\n",
    "years = [2019, 2020, 2021, 2022, 2023]\n",
    "months = ALL_MONTHS\n",
    "\n",
    "T = len(years) * len(months)\n",
    "\n",
    "# syntagm = \"fake news\"\n",
    "syntagm = \"lockdown\"\n",
    "\n",
    "data = computeData(syntagm, years, months)\n",
    "\n",
    "pi = np.array([data[FREQ_SUM_SYNTAGMS], data[FREQ_ALL_SYNTAGM]])\n",
    "\n",
    "print(pi)\n",
    "print()\n",
    "\n",
    "res = ER(T, pi, 0.01, 20)\n",
    "\n",
    "maximo = 0\n",
    "ite = 0\n",
    "for t, r in enumerate(res):\n",
    "    if r > maximo:\n",
    "        maximo = r\n",
    "        ite = t\n",
    "    print(t, \"-->\", \"%5.24f\" % r)\n",
    "\n",
    "year = math.floor(ite / len(months))\n",
    "month = ite % len(months)\n",
    "print(f\"Mayor probabilidad en {years[year]}/{MONTHS[month - 1]} con {maximo}\")\n",
    "\n",
    "# # res_s = ER_santini(T, pi, 0.01, 20)\n",
    "\n",
    "# for r, rs in zip(res, res_s):\n",
    "#     print(\"%5.3f\" % r == \"%5.3f\" % rs, \"%5.3f\" % r, \"%5.3f\" % rs)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
