%
% Options:
%   [DRAFT]: (default) one column single spaced
%
%
%[DRAFT]
\documentclass[10pt]{article}
\usepackage{amsmath,amsopn,amscd,amsthm,amssymb,xypic,rotating,epic}
%[~DRAFT]


\xyoption{all}

%[DRAFT]
\renewcommand{\familydefault}{cmtt}
\parindent=4em
\parskip=1em
\raggedbottom
\raggedright
\oddsidemargin  -0.3 in
\textwidth      7   in
\topmargin      0.  in
\textheight     8.2 in 

\newcommand{\double}       {\baselineskip 11pt}
\newcommand{\single}       {\baselineskip 11pt}

\pagestyle{myheadings}
\markboth{}{syntagm formation}
%[~DRAFT]

\newtheorem{definition}{Definition}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}{Lemma}[section]
\newcommand{\obs}[1] {\begin{center}\framebox{\parbox{0.75\columnwidth}{\textbf{#1}}}\end{center}}

% -----------------------------------------------
%   M A C R O S
% 
% Macros for inserting figures
%
\newcommand{\VInsert}[2]   {\centerline{\immediate\pdfximage height #2 {#1}\pdfrefximage\pdflastximage}}
\newcommand{\HInsert}[2]   {\centerline{\immediate\pdfximage width #2  {#1}\pdfrefximage\pdflastximage}}


%
% Macros for algorithms
%
\newcommand{\cmd}[1]      {\underline{{#1}}}
\newcommand{\cb}          {\begin{tabbing}MMMMM\=MM\=MM\=MM\=MM\=MM\=MM\=MM\=MM\=MM\= \kill}
\newcommand{\ce}          {\end{tabbing}}

\def\eoe{\\\hfill(end of example)\\\bigskip}

\def\eor{\\\hfill(end of remark)}


%
% Common macros for data types
% 
\newcommand{\tset}[1]      {\{{#1}\}}                     % set data type
%\newcommand{\tbag}[1]      {\{\!\!|{#1}\}\!\!\!|}         % bag data type
\newcommand{\tbag}[1]      {\{\hspace{-0.25em}|{#1}\}\hspace{-0.5em}|}         % bag data type
\newcommand{\tlist}[1]     {[{#1}]}                       %list data type
\newcommand{\ttree}[1]     {{\frak t}({#1})}              %tree data type

\newcommand{\separate}     {\vspace{0.3cm}\begin{center}*~~~~~~~~~~*~~~~~~~~~~*\end{center}\vspace{0.3cm}}
\newcommand{\dqt}[1]        {"{#1}"}
\newcommand{\bp}            {\rule{5em}{0pt}}
\def\emph{\textsl}
\def\em{\sl}
\def\textbf{\pmb}
\def\dfeq{\stackrel{\triangle}{=}}


%
% Macros for revision and off-the-text definitions 
%
\newcommand{\revise}       {\marginpar{\textbf{\begin{rotate}{90}revise?\end{rotate}}}}
\newcommand{\comment}[1]   {\marginpar{\textbf{\begin{rotate}{90}{#1}\end{rotate}}}}
%
% Places a definition in the margin
%
\newcommand{\maronly}[1]    {\marginpar{\framebox{\parbox{7em}{\small #1}}}}

%
% Defines a term putting it in boldface and repeats it at the margin. The alternative
% definition is for wide text without space in the margins: just puts it in bold
%
\newcommand{\defmar}[1]    {\textbf{#1}\marginpar{\framebox{\parbox{7em}{\small #1}}}}
%\newcommand{\defmar}[1]    {\textbf{#1}}



%
% -----------------------------------------------



% -----------------------------------------------
% Macros specific to this file
%
\newcommand{\synt}[1]     {\lceil{#1}\rfloor}
\def\st{s}
\def\ev{e}


\newcounter{myremark}
\newcounter{myexample}
\setcounter{myremark}{0}
\setcounter{myexample}{0}

\def\example{
\bigskip

%\addtocounter{myexample}{1}%
\refstepcounter{myexample}%
\noindent \textbf{Example \Roman{myexample}:}\\
}


\def\remark{
%\addtocounter{myremark}{1}%
\refstepcounter{myremark}%
\noindent \emph{Remark \arabic{myremark}:}
}

\renewcommand{\topfraction}{.99}
\renewcommand{\textfraction}{.01}


\begin{document}

%[DRAFT]
\title{Document title}
\author{Simone Santini}
\date{Universidad Aut\'onoma de Madrid}
%%[~DRAFT]

\section{The Premises}
The starting point is to create a simple model of the generation of
syntagms in social media. The general idea is the following.

We are \dqt{chasing} a specific syntagm $\synt{w_1\cdots{w_n}}$ that
we have observed being present at the end of the period of observation
using the method in \cite{shang:2018}. We observe the comunication during a
period $T=[1,\ldots,n]$: we asume that at $t=1$ the syntagm has not
been created, and at $t=n$ has been. We are interested in determining
its evolution, in particular, in determining the time $t$ at which we
can assume that the syntagm has been created. Our observations are a
sequence of documents $D_t$, $t=1,\ldots,n$, one for each time step
(this will be in general the union of various documents, for example,
all the tweets written in a certain group on a given week; for the
purpose of our discussion, we merge all the documents into one); the
words contained in the documents will form our measurements, as we
shall see later on.


Let us place ourselves at a time $t$ at which the syntagm has not yet
emerged. The creation of the syntagm is due to the ocurrence of an
external \emph{event} (for example, the syntagm \dqt{fake news} was
related to the beginning of the 2016 presidential campaign). The event
is something that happens at a given point in time and does not
necessarily repeat itself. On the other hand, the event creates a
\emph{situation}, that is potentially self-supporting in time that is,
it may be present after the end of the event. Let is consider two
stochastic processes, $\ev_t$ and $s_t$. The first is the variable
that models the ocurrence of the event at time $t$, the second is the
variable that models the existence of a situation conducive to the
creation of the syntagm at time $t$. Both take valoes in $\{0,1\}$
($1$ means that the event ocurred or the situation is present, $0$
means the opposite). The existence of the situation at time $t$
depends on the existence at time $t-1$ or the ocurrence of the event
at time $t$. The diagram representing the dependence of the randon
variables at time $t$ is the following:
%
%
\begin{equation}
  \xymatrix{
    & *++[o][F]{\ev_t} \ar[d] \\
    *++[o][F]{\st_{t-1}} \ar[r] & *++[o][F]{\st_t} 
  }
\end{equation}
%
%
The existence of the situation detemines the propability of observing
the meaningful syntagm. Let $w_t$ be the event of the syntagm being
formed (again, $w_t=1$ if the syntagm is present, $w_t=0$ if
not). The presence of the syntagm depends directly only on the
existence of the \emph{situation} at time $t$. We have therefore the
following scheme:
%
%
\begin{equation}
  \xymatrix{
    & *++[o][F]{\ev_t} \ar[d] \\
    *++[o][F]{\st_{t-1}} \ar[r] & *++[o][F]{\st_t} \ar[d] \\
    & *++[F]{w_t}
  }
\end{equation}
%
%
(The square indicates the element that we observe). Here, to use the
notation of \cite{shang:2018}, it is
%
%
\begin{equation}
  w = \synt{w_1,\ldots,w_n}
\end{equation}
%
%
that is $w$ is the syntagm that we are chasing, composed of $n$ words.

Note that we are fixating on a single syntagm, which we assume to have
identified at the end of the temporal sequence, and we are interested
in seeing where it was generated estimating $\ev_t$ in such a way to
explain the observations. At time $t$, we make an observation $\pi_t$
(we shall see later how this is actually determined). We actually
needs two measures, one that determines the information in favor of
the hypothesis that at time $t$ the syntagm was not expressed (call it
$\pi_t^0$) and the other expressing the information in favor of the
hypothesis that the syntagm was expressed. The direct dependence of
$w$ (we shall omit the inex $t$ when no confusion arises) is on $\st$,
so we can write
%
%
\begin{equation}
  P(w=i) = \sum_j P(w=i|\st=j)P(\st=j)
\end{equation}
%
%
or, developing further
%
%
\begin{equation}
  P(w_t=i) = \sum_j \sum_k \sum_h P\bigl[w_t=i\bigl|\st_t=j\bigr]P\bigl[\st_t=j\bigl|\st_{t-1}=k,\ev_t=h\bigr]P\bigl[\st_{t-1}=k\bigr]P\bigl[\ev_t=h\bigl]
\end{equation}
%
%
The observed variable $w$ (the syntagm) depends on the \emph{latent
  variables}
%
%
\begin{equation}
  \zeta_t = (\st_t, \st_{t-1}, \ev_t)
\end{equation}
%
%
which are the ones whose probability distribution, at each time $t$,
we have to determine. The problem is solved by minimizing the functional
%
%
\begin{equation}
  \begin{aligned}
    {\mathcal{L}}(\theta) &= \sum_i \pi_t^i \log P(w_t=i|\theta) \\
    &= \sum_i \pi_t^i \sum_{j,k,h} P\bigl[w_t=i\bigl|\st_t=j\bigr]P\bigl[\st_t=j\bigl|\st_{t-1}=k,\ev_t=h\bigr]P\bigl[\st_{t-1}=k\bigr]P\bigl[\ev_t=h\bigl]
  \end{aligned}
\end{equation}
%
%
Here
%
%
\begin{equation}
  \theta = (P\bigl[w_t=i\bigl|\st_t=j\bigr], P\bigl[\st_t=j\bigl|\st_{t-1}=k,\ev_t=h\bigr], P\bigl[\st_{t-1}=k\bigr], P\bigl[\ev_t=h\bigl])
\end{equation}
%
%
are the parameters over which we must optimize. Since we have to
manipulate these parameters, it is convenient to use a simpler
notation. Define
%
%
\begin{equation}
  \begin{aligned}
    \alpha_j^i(t) &= P\bigl[w_t=i\bigl|\st_t=j\bigr] \\
    \beta_{kh}^j(t) &= P\bigl[\st_t=j\bigl|\st_{t-1}=k,\ev_t=h\bigr] \\
    \nu^k(t) &= P\bigl[\st_{t-1}=k\bigr] \\
    \gamma^k(t) &= P\bigl[\ev_t=h\bigl])
  \end{aligned}
\end{equation}
%
%
Note that we put in superscript the value of the conditioned variable
and in subscript that of the condintioning variable(s). 


\section{The method}
We use the general method of the EM (Expectation Maximization)
algorithm \cite{dempster:77}. I will skip over it a bit; if necessary I will put more
details in a following draft. The function ${\mathcal{L}}$ is bounded
from below by
%
%
\begin{equation}
  g(\theta,\theta') = 
  \sum_i \pi^i {\mathbb{E}}_{\zeta|w=i,\theta'}\bigl[\log P(w=i,\zeta=(j,k,h)|\theta\bigr] + \sum_i \pi^i H\bigl[P(\zeta|\theta')]
\end{equation}
%
%
Here, during the iteration, $\theta'$ is the set of parameters
determined at the previous step, and $\theta$ the set we solve
for. the second term is independent of $\theta$, so we can optimize
%
%
\begin{equation}
  \begin{aligned}
    Q &= \sum_i \pi^i {\mathbb{E}}_{\zeta|w=i,\theta'}\bigl[\log P(w=i,\zeta=(j,k,h)|\theta\bigr] \\
    &= \sum_i \pi^i \sum_{j,k,h} P(\zeta=(j,k,h)|w=i, \theta') \log P(w=i,\zeta=(j,k,h)|\theta) \\
    &= \sum_i \pi^i  \sum_{j,k,h} P(\zeta=(j,k,h)|w=i, \theta') \log P\bigl[w_t=i\bigl|\st_t=j\bigr]P\bigl[\st_t=j\bigl|\st_{t-1}=k,\ev_t=h\bigr]P\bigl[\st_{t-1}=k\bigr]P\bigl[\ev_t=h\bigr] \\
    &= \sum_{i,j,k,h} \pi^i \tau_i^{jkh} \log \alpha_j^i \beta_{kh}^j \nu^k \gamma^h
  \end{aligned}
\end{equation}
%
%
where we have set 
%
%
\begin{equation}
  \tau_i^{jkh} = P(\st_t=j, \st_{t-1}, \ev_t=h | w=i)
\end{equation}
%
%
In the \textbf{E} step, we determine $Q$, that is, in practice, we
determine $\tau_i^{jkh}$, which is given by
%
%
\begin{equation}
  \label{eeq}
  \tau_i^{jkh} = \frac{\alpha_j^i \beta_{kh}^j \nu^k \gamma^h}
                     {\displaystyle \sum_{jkh} \alpha_j^i \beta_{kh}^j \nu^k \gamma^h}
\end{equation}
%
%
For the \textbf{M} step we maximuze $Q$ subject to the constraints
%
%
\begin{equation}
  \sum_i \alpha_j^i = 1\ \ \ \ \ \sum_j \beta_{kh}^j = 1\ \ \ \ \ \sum_h \gamma^h = 1
\end{equation}
%
%
We apply the Lagrange multipliers and minimize
%
%
\begin{equation}
  {\mathcal{F}} = \sum_{i,j,k,h} \pi^i \tau_i^{jkh} \log \alpha_j^i \beta_{kh}^j \nu^k \gamma^h 
  - \sum_j \lambda_j \left(\sum_i \alpha_j^i - 1\right) 
  - \sum_{k,h} \mu_{kh} \left( \sum_j \beta_{kh}^j - 1 \right) 
  - \epsilon \left( \sum_h \gamma^h - 1\right)
\end{equation}
%
%
We detemine the parameters by setting to zero the derivatives of
${\mathcal{F}}$
%
%
\begin{center}
  \begin{tabular}{lcl}
  $\displaystyle \frac{\partial {\mathcal{F}}}{\partial \alpha_j^i} = 
    \sum_{hk} \tau_i^{jkh} \pi^i \frac{1}{\alpha_j^i} - \lambda_j = 0$
    &  \rule{5em}{0pt} & 
    $\displaystyle \alpha_j^i = \frac{\pi^i}{\lambda_j} \sum_{k,h} \tau_i^{jkh}$ \\
    $\displaystyle \frac{\partial {\mathcal{F}}}{\partial \beta_{hk}^j} = 
    \sum_i \tau_i^{jkh} \pi^i \frac{1}{\beta_{kh}^j} - \mu_{kh} = 0$
    &  &
    $\displaystyle \beta_{kh}^j = \frac{1}{\mu_{kh}} \sum_i \tau_i^{jkh} \pi^i$ \\
    $\displaystyle \frac{\partial {\mathcal{F}}}{\partial \gamma^h} = 
    \sum_{ijk} \tau_i^{jkh} \pi^i \frac{1}{\gamma^h} - \epsilon = 0$
    & &
    $\displaystyle \gamma^h = \frac{1}{\epsilon} \sum_{ijk} \tau_i^{jkh} \pi^i$
  \end{tabular}
\end{center}

The Lagrange multipliers are determined by normalization
%
%
\begin{equation}
  \begin{aligned}
    \alpha_j^i &= \frac{\displaystyle \pi^i \sum_{kh} \tau_i^{jkh}}{\displaystyle \sum_i \pi^i \sum_{kh} \tau_i^{jkh}} \\
    \beta_{kh}^j &= \frac{\displaystyle \sum_i \pi^i \tau_i^{jkh}}{\displaystyle \sum_{i,j} \pi^i \tau_i^{jkh}} \\
    \gamma^h &= \frac{\displaystyle \sum_{ijk} \pi^i \tau_i^{jkh}}{\displaystyle \sum_{i,j,k,h} \pi^i \tau_i^{jkh}} 
  \end{aligned}
\end{equation}
%
%
With the new vales of the parameters we calculate $\tau_i^{jkh}$ using
(\ref{eeq}), then new values of the parameters, etc.\ until
convergence. This gives, among other things, the value
$\gamma^1=P(\ev_t=1)$, which gives us the probability that the event
that caused the syntagm to be created happened at time $t$. 

The calculation depends on the value $\nu^k$, which is equal to
$P(\st_{t-1}=h)$. In order to run the approximation over a span of
time, it is necessary to compute $P(\st_t=h)$. From 
%
%
\begin{equation}
  P(\st_t=j) = \sum_{k,h} P[\st_t=j|\st_{t-1}=k,\ev_t=h] P[\st_{t-1}=k] P[\ev_t=h]
\end{equation}
%
%
we derive
%
%
\begin{equation}
  \nu^j(t) = \sum_{k,h} \beta_{kh}^j \nu^k(t-1) \gamma^h
\end{equation}
%
%



\section{The algorithm}
We consider a time span $\Delta=[1,\ldots,T]$ over which we want to
determine the emergence of the syntagm. At each time $t$ we have a set
of document, which we use to determine the measurements $\pi^i(t)$
($i\in\{0,1\}$) that determine the information supporting the
hypothesis that the syntagm is not present ($\pi^0(t)$) and that is
present ($\pi^1(t)$). We shall consider how to determine $\pi^i(t)$ in
the next section. For the moment, let us consider that a sequence
$[\pi^i(1),\ldots,\pi^i(t)]$ is avaiable. 

We run the estimation time-wise, from $t=1$ to $t=T$, using at each
time the values $\nu^j(t)$ in order to run the computation at
$t+1$. In order to do this, we must initialize $\nu^h(0)$. We can
assume that the situation is not present at time $0$ and set
$\nu^0(0)=1$, $\nu^1(0)=0$, or use a small probability $p$ for the
existence of the situation. The algorithm is in Figure~\ref{algo}:


Convergence is determined by the stabilization of the parameters, any
number of criteria that measure the change in the parameters between
iterations can be used. The list $\Psi$ contains, for each time $t$,
the probability that the event that caused the syntagm to be generated
happened at time $t$.

\section{The measure}
The final piece of the puzzle that has to be put into place is the
measurement $\pi^i(t)$. We have defined it informqlly as the
information that supports the hypothesis that the syntagm has been
created (resp., has not been created). This is quite a fuzzy
definition, and probably can be satisfied in many different ways. This
is one possibility.

We are \dqt{chasing} a syntagm $\synt{w_1\cdots{w_n}}$ and, as
evidence, we have, at each time $t$, a document $D_t$. The data that
we extract are essentially:
%
\begin{description}
\item[i)] the number of observations of the individual words
  $n(w_1),\ldots,n(w_n)$;
\item[ii)] the number of observations of the whole sequence of words
  that composes the syntagm $n(w_1,\ldots,w_n)$;
\item[iii)] the \dqt{quality} of the syntagm,
  $q(\synt{w_1\cdots{w_n}})$, determined as in \cite{shang:2018}.
\end{description}

The general idea is that if we see many ocurrences of the whole
syntagm ($n(w_1,\ldots,w_n)$ high) and the syntagm is significant
($q(\synt{w_1\cdots{w_n}})$ high), then there is evidence of its
presence ($\pi^1(t)$ high, $\pi^0(t)$ low). Vice versa, if the words
are observed individually but not as a whole ($n(w_1),\ldots,n(w_n)$
high, $n(w_1,\ldots,w_n)$ low) or the whole is not a viable syntagm
($q(\synt{w_1\cdots{w_n}})$ low), then the evidence supports the
absence of the syntagm ($\pi^0(t)$ high, $\pi^1(t)$ low).

So, OK, without further ado, one possibility is
%
%
\begin{equation}
  \begin{aligned}
    \pi^1 &= \begin{cases}
      q(\synt{w_1\cdots{w_n}}) \frac{\displaystyle
        n(w_1,\ldots,w_n)}{\displaystyle \max(n(w_1),\ldots,n(w_n))} & \mbox{if $\max(n(w_1),\ldots,n(w_n))>0$} \\
      0 & \mbox{otherwise} 
      \end{cases} \\
    \pi^0 &= 1 - \pi^1
  \end{aligned}
\end{equation}
%
%
Note that if $\max(n(w_1),\ldots,n(w_n))=0$ (no words of the syntagm
are observed), we consider that there is sure evidence of the absence
of the syntagm. This might not be the best choice: one can consider
this case neutral ($\pi^1=1/2$) or even discard the information at
that time. There things should be tried out.

Quite honestly, this is the part I feel less sure about. The general
idea of the information $\pi^i(t)$ is quite clear, as are the general
characteristics that these coefficients should have vis-\'a-vis the
observations, but the specific form that these function should have is
still something to be worked out.
%
%
\begin{figure}[htbp]
  {\tt
    \rule{\textwidth}{0.5pt}
    \cb
    1.  \> $\Psi \leftarrow []$ \\
    2.  \> \cmd{for} t $\leftarrow$ 1 \cmd{to} $T$ \cmd{do} \\
    3.  \> \> initialize $\alpha_j^i$, $\beta_{kh}^j$, $\gamma^h$, $\tau_i^{jkh}$ \\
    4.  \> \> \cmd{while} \cmd{not} convergence \cmd{do} \\
    5.  \> \> \> $\displaystyle   \tau_i^{jkh} \leftarrow \frac{\alpha_j^i \beta_{kh}^j \nu^k(t-1) \gamma^h}
                                                {\displaystyle \sum_{jkh} \alpha_j^i \beta_{kh}^j \nu^k(t-1) \gamma^h}$ ($i,j,k,h\in\{0,1\}$) \\
    6.  \> \> \> $\displaystyle \alpha_j^i \leftarrow \frac{\displaystyle \pi^i(t) \sum_{kh} \tau_i^{jkh}}{\displaystyle \sum_i \pi^i(t) \sum_{kh} \tau_i^{jkh}}$ ($i,j\in\{0,1\}$) \\
    7.  \> \> \> $\displaystyle \beta_{kh}^j \leftarrow \frac{\displaystyle \sum_i \pi^i(t) \tau_i^{jkh}}{\displaystyle \sum_{i,j} \pi^i(t) \tau_i^{jkh}}$ ($j,k,h\in\{0,1\}$) \\
    8.  \> \> \> $\gamma^h \leftarrow \frac{\displaystyle \sum_{ijk} \pi^i(t) \tau_i^{jkh}}{\displaystyle \sum_{i,j,k,h} \pi^i(t) \tau_i^{jkh}}$ ($h\in\{0,1\}$) \\
    9.  \> \> \cmd{od} \\
    10. \> \> $\displaystyle \nu^j(t) = \sum_{k,h} \beta_{kh}^j \nu^k(t-1) \gamma^h$ ($j\in\{0,1\}$) \\
    11. \> \> $\Psi \leftarrow \Psi + [\gamma^1]$ \\
    12. \> \cmd{od}
    \ce
  }
  \caption{The algorithm for the determination of the probability of
    having the syntagm-creating event.}
  \label{algo}
  \rule{\textwidth}{0.5pt}
\end{figure}

\bibliographystyle{plain}
% \bibliography{/Users/ssantini/Documents/work/biblio/database,/Users/ssantini/Documents/work/biblio/santini,/Users/ssantini/Documents/work/biblio/personal/books}
%\bibliography{G:/Documents/work/biblio/database,G:/Documents/work/biblio/santini,G:/Documents/work/biblio/personal/books}
%\bibliography{database,santini,books}


\end{document}
